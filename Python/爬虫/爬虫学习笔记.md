# 爬虫学习笔记

爬虫的基本概念

数据解析

高性能异步爬虫

elenium模块的基本使用

scrapy框架

requests库

# 爬虫的基本概念

## 爬虫的主要流程

构造url

爬虫要爬的数据，绝不仅仅是一个网页那么简单，有时候我们需要爬的是整个网站的数据，如果我们一个一个网页来获取url，那效率肯定太低了。所以在写爬虫程序之前，需要先知道url地址的规律，这样子才可以构造url列表，再从url列表中去url去爬我们需要的数据。

发送请求，获取响应

通过HTTP库向目标站点发起请求，也就是发送一个Request等待服务器响应，如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HTML,Json字符串，二进制数据（图片或者视频）等类型。

提取数据
返回的数据时html时，我们可以用正则表达式，或者是lxml模块配合xpath提取数据；返回的是json字符串时，我们可以用json模块进行数据解析；返回的是二进制数据时，可以做保存或者进一步的处理。

## 爬虫分类

-通用爬虫

-聚焦爬虫

-增量爬虫

## 反爬手段

-UA检测

-验证码

-证书验证 verify = false

-cookies

## request

## reponse

    响应状态（状态码）
    响应头
    响应体
    
robots.txt协议：

    规定了网站中哪些数据可以被爬虫爬取哪些数据不可以被爬取。

http协议(超文本传输协议)

    - 概念：就是服务器和客户端进行数据交互的一种形式。

常用请求头信息

    - User-Agent：请求载体的身份标识
    - Connection：请求完毕后，是断开连接还是保持连接

常用响应头信息

    - Content-Type：服务器响应回客户端的数据类型

https协议：

    - 安全的超文本传输协议

加密方式

    - 对称秘钥加密 
    - 非对称秘钥加密
    - 证书秘钥加密




如何使用：（requests模块的编码流程）
    
- 指定url
        
- UA伪装
       - 请求参数的处理
    
- 发起请求
    
- 获取响应数据
    
- 持久化存储

环境安装：
    
pip install 
requests


- 动态加载数据
        
- 首页中对应的企业信息数据是通过ajax动态请求到的。

        http://125.35.6.84:81/xk/itownet/portal/dzpz.jsp?id=e6c1aa332b274282b04659a6ea30430a
        http://125.35.6.84:81/xk/itownet/portal/dzpz.jsp?id=f63f61fe04684c46a016a45eac8754fe
        - 通过对详情页url的观察发现：
            
- url的域名都是一样的，只有携带的参数（id）不一样
  - id值可以从首页对应的ajax请求到的json串中获取
           
- 域名和id值拼接处一个完整的企业对应的详情页的url
        
- 详情页的企业详情数据也是动态加载出来的
           
- http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById
            
- http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById
            
- 观察后发现：
                
- 所有的post请求的url都是一样的，只有参数id值是不同。
                
- 如果我们可以批量获取多家企业的id后，就可以将id和url形成一个完整的详情页对应详情数据的ajax请求的url

# 数据解析
 
聚焦爬虫:爬取页面中指定的页面内容。

数据解析分类：

    - 正则
    - bs4
    - xpath（***）

数据解析原理概述：

    - 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储
    - 1.进行指定标签的定位
    - 2.标签或者标签对应的属性中存储的数据值进行提取（解析）

正则解析：

    <div class="thumb">

    <a href="/article/121721100" target="_blank">
    <img src="//pic.qiushibaike.com/system/pictures/12172/121721100/medium/DNXDX9TZ8SDU6OK2.jpg" alt="指引我有前进的方向">
    </a>

    </div>

    ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'


bs4进行数据解析

    - 数据解析的原理：
        - 1.标签定位
        - 2.提取标签、标签属性中存储的数据值
    - bs4数据解析的原理：
        - 1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中
        - 2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取
    - 环境安装：
        - pip install bs4
        - pip install lxml
    - 如何实例化BeautifulSoup对象：
        - from bs4 import BeautifulSoup
        - 对象的实例化：
            - 1.将本地的html文档中的数据加载到该对象中
                    fp = open('./test.html','r',encoding='utf-8')
                    soup = BeautifulSoup(fp,'lxml')
            - 2.将互联网上获取的页面源码加载到该对象中
                    page_text = response.text
                    soup = BeatifulSoup(page_text,'lxml')
        - 提供的用于数据解析的方法和属性：
            - soup.tagName:返回的是文档中第一次出现的tagName对应的标签
            - soup.find():
                - find('tagName'):等同于soup.div
                - 属性定位：
                    -soup.find('div',class_/id/attr='song')
            - soup.find_all('tagName'):返回符合要求的所有标签（列表）
        - select：
            - select('某种选择器（id，class，标签...选择器）'),返回的是一个列表。
            - 层级选择器：
                - soup.select('.tang > ul > li > a')：>表示的是一个层级
                - oup.select('.tang > ul a')：空格表示的多个层级
        - 获取标签之间的文本数据：
            - soup.a.text/string/get_text()
            - text/get_text():可以获取某一个标签中所有的文本内容
            - string：只可以获取该标签下面直系的文本内容
        - 获取标签中属性值：
            - soup.a['href']

xpath解析：最常用且最便捷高效的一种解析方式。通用性。

    - xpath解析原理：
        - 1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。
        - 2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。
    - 环境的安装：
        - pip install lxml
    - 如何实例化一个etree对象:from lxml import etree
        - 1.将本地的html文档中的源码数据加载到etree对象中：
            etree.parse(filePath)
        - 2.可以将从互联网上获取的源码数据加载到该对象中
            etree.HTML('page_text')
        - xpath('xpath表达式')
    - xpath表达式:
        - /:表示的是从根节点开始定位。表示的是一个层级。
        - //:表示的是多个层级。可以表示从任意位置开始定位。
        - 属性定位：//div[@class='song'] tag[@attrName="attrValue"]
        - 索引定位：//div[@class="song"]/p[3] 索引是从1开始的。
        - 取文本：
            - /text() 获取的是标签中直系的文本内容
            - //text() 标签中非直系的文本内容（所有的文本内容）
        - 取属性：
            /@attrName     ==>img/src

验证码识别

反爬机制：验证码.识别验证码图片中的数据，用于模拟登陆操作。

识别验证码的操作：
 第三方自动识别（推荐）
        - 云打码：http://www.yundama.com/demo.html
云打码的使用流程：
    - 注册：普通和开发者用户
    - 登录：
        - 普通用户的登录：查询该用户是否还有剩余的题分
        - 开发者用户的登录：
            - 创建一个软件：我的软件-》添加新软件-》录入软件名称-》提交（软件id和秘钥）
            - 下载示例代码：开发文档-》点此下载：云打码接口DLL-》PythonHTTP示例下载

使用打码平台识别验证码的编码流程：

    - 将验证码图片进行本地下载
    - 调用平台提供的示例代码进行图片数据识别

模拟登录：爬取基于某些用户的用户信息。

http/https协议特性：无状态。
没有请求到对应页面数据的原因：
    发起的第二次基于个人主页页面请求的时候，服务器端并不知道该此请求是基于登录状态下的请求。
cookie：用来让服务器端记录客户端的相关状态。
    - 手动处理：通过抓包工具获取cookie值，将该值封装到headers中。（不建议）
    - 自动处理：
        - cookie值的来源是哪里？
            - 模拟登录post请求后，由服务器端创建。
        session会话对象：
            - 作用：
                1.可以进行请求的发送。
                2.如果请求过程中产生了cookie，则该cookie会被自动存储/携带在该session对象中。
        - 创建一个session对象：session = requests.Session()
        - 使用session对象进行模拟登录post请求的发送（cookie就会被存储在session中）
        - session对象对个人主页对应的get请求进行发送（携带了cookie）

代理：破解封IP这种反爬机制。
什么是代理：
    - 代理服务器。
代理的作用：
    - 突破自身IP访问的限制。
    - 隐藏自身真实IP
代理相关的网站：
    - 快代理
    - 西祠代理
    - www.goubanjia.com
代理ip的类型：
    - http：应用到http协议对应的url中
    - https：应用到https协议对应的url中

代理ip的匿名度：
    - 透明：服务器知道该次请求使用了代理，也知道请求对应的真实ip
    - 匿名：知道使用了代理，不知道真实ip
    - 高匿：不知道使用了代理，更不知道真实的ip
    
# 高性能异步爬虫

目的：在爬虫中使用异步实现高性能的数据爬取操作。

异步爬虫的方式：
    - 1.多线程，多进程（不建议）：
        好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。
        弊端：无法无限制的开启多线程或者多进程。
    - 2.线程池、进程池（适当的使用）：
        好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。
        弊端：池中线程或进程的数量是有上限。

- 3.单线程+异步协程（推荐）：
    event_loop：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，
    当满足某些条件的时候，函数就会被循环执行。

    coroutine：协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。
    我们可以使用 async 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回
    一个协程对象。

    task：任务，它是对协程对象的进一步封装，包含了任务的各个状态。

    future：代表将来执行或还没有执行的任务，实际上和 task 没有本质区别。

    async 定义一个协程.

    await 用来挂起阻塞方法的执行。

# elenium模块的基本使用

问题：selenium模块和爬虫之间具有怎样的关联？
    - 便捷的获取网站中动态加载的数据
    - 便捷实现模拟登录
什么是selenium模块？
    - 基于浏览器自动化的一个模块。

selenium使用流程：
    - 环境安装：pip install selenium
    - 下载一个浏览器的驱动程序（谷歌浏览器）
        - 下载路径：http://chromedriver.storage.googleapis.com/index.html
        - 驱动程序和浏览器的映射关系：http://blog.csdn.net/huilan_same/article/details/51896672
    - 实例化一个浏览器对象
    - 编写基于浏览器自动化的操作代码
        - 发起请求：get(url)
        - 标签定位：find系列的方法
        - 标签交互：send_keys('xxx')
        - 执行js程序：excute_script('jsCode')
        - 前进，后退：back(),forward()
        - 关闭浏览器：quit()

    - selenium处理iframe
        - 如果定位的标签存在于iframe标签之中，则必须使用switch_to.frame(id)
        - 动作链（拖动）：from selenium.webdriver import ActionChains
            - 实例化一个动作链对象：action = ActionChains(bro)
            - click_and_hold（div）：长按且点击操作
            - move_by_offset(x,y)
            - perform()让动作链立即执行
            - action.release()释放动作链对象

12306模拟登录
    - 超级鹰：http://www.chaojiying.com/about.html
        - 注册：普通用户
        - 登录：普通用户
            - 题分查询：充值
            - 创建一个软件（id）
            - 下载示例代码

    - 12306模拟登录编码流程：
        - 使用selenium打开登录页面
        - 对当前selenium打开的这张页面进行截图
        - 对当前图片局部区域（验证码图片）进行裁剪
            - 好处：将验证码图片和模拟登录进行一一对应。
        - 使用超级鹰识别验证码图片（坐标）
        - 使用动作链根据坐标实现点击操作
        - 录入用户名密码，点击登录按钮实现登录

# scrapy框架

- 什么是框架？
    - 就是一个集成了很多功能并且具有很强通用性的一个项目模板。

- 如何学习框架？
    - 专门学习框架封装的各种功能的详细用法。

- 什么是scrapy？
    - 爬虫中封装好的一个明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式

- scrapy框架的基本使用
    - 环境的安装：
        - mac or linux：pip install scrapy
          - windows:
            - pip install wheel
            - 下载twisted，下载地址为http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
            - 安装twisted：pip install Twisted‑17.1.0‑cp36‑cp36m‑win_amd64.whl
            - pip install pywin32
            - pip install scrapy
            测试：在终端里录入scrapy指令，没有报错即表示安装成功！
    - 创建一个工程：scrapy startproject xxxPro
    - cd xxxPro
    - 在spiders子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.xxx.com
    - 执行工程：
        - scrapy crawl spiderName

- scrapy数据解析

- scrapy持久化存储
    - 基于终端指令：
        - 要求：只可以将parse方法的返回值存储到本地的文本文件中
        - 注意：持久化存储对应的文本文件的类型只可以为：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle
        - 指令：scrapy crawl xxx -o filePath
        - 好处：简介高效便捷
        - 缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）

    - 基于管道：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据封装存储到item类型的对象
            - 将item类型的对象提交给管道进行持久化存储的操作
            - 在管道类的process_item中要将其接受到的item对象中存储的数据进行持久化存储操作
            - 在配置文件中开启管道
        - 好处：
            - 通用性强。

    - 面试题：将爬取到的数据一份存储到本地一份存储到数据库，如何实现？
        - 管道文件中一个管道类对应的是将数据存储到一种平台
        - 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接受
        - process_item中的return item表示将item传递给下一个即将被执行的管道类


- 基于Spider的全站数据爬取
    - 就是将网站中某板块下的全部页码对应的页面数据进行爬取
    - 需求：爬取校花网中的照片的名称
    - 实现方式：
        - 将所有页面的url添加到start_urls列表（不推荐）
        - 自行手动进行请求发送（推荐）
            - 手动请求发送：
                - yield scrapy.Request(url,callback):callback专门用做于数据解析

- 五大核心组件
    引擎(Scrapy)
        用来处理整个系统的数据流处理, 触发事务(框架核心)
    调度器(Scheduler)
        用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
    下载器(Downloader)
        用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
    爬虫(Spiders)
        爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
    项目管道(Pipeline)
        负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。



- 请求传参
    - 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
    - 需求：爬取boss的岗位名称，岗位描述

- 图片数据爬取之ImagesPipeline
    - 基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别？
        - 字符串：只需要基于xpath进行解析且提交管道进行持久化存储
        - 图片：xpath解析出图片src的属性值。单独的对图片地址发起请求获取图片二进制类型的数据

    - ImagesPipeline：
        - 只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据，且还会帮我们进行持久化存储。
    - 需求：爬取站长素材中的高清图片
    - 使用流程：
        - 数据解析（图片的地址）
        - 将存储图片地址的item提交到制定的管道类
        - 在管道文件中自定制一个基于ImagesPipeLine的一个管道类
            - get_media_request
            - file_path
            - item_completed
        - 在配置文件中：
            - 指定图片存储的目录：IMAGES_STORE = './imgs_bobo'
            - 指定开启的管道：自定制的管道类


- 中间件
    - 下载中间件
        - 位置：引擎和下载器之间
        - 作用：批量拦截到整个工程中所有的请求和响应
        - 拦截请求：
            - UA伪装:process_request
            - 代理IP:process_exception:return request

        - 拦截响应：
            - 篡改响应数据，响应对象
            - 需求：爬取网易新闻中的新闻数据（标题和内容）
                - 1.通过网易新闻的首页解析出五大板块对应的详情页的url（没有动态加载）
                - 2.每一个板块对应的新闻标题都是动态加载出来的（动态加载）
                - 3.通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻内容

- CrawlSpider:类，Spider的一个子类
    - 全站数据爬取的方式
        - 基于Spider：手动请求
        - 基于CrawlSpider
    - CrawlSpider的使用：
        - 创建一个工程
        - cd XXX
        - 创建爬虫文件（CrawlSpider）：
            - scrapy genspider -t crawl xxx www.xxxx.com
            - 链接提取器：
                - 作用：根据指定的规则（allow）进行指定链接的提取
            - 规则解析器：
                - 作用：将链接提取器提取到的链接进行指定规则（callback）的解析
        #需求：爬取sun网站中的编号，新闻标题，新闻内容，标号
            - 分析：爬取的数据没有在同一张页面中。
            - 1.可以使用链接提取器提取所有的页码链接
            - 2.让链接提取器提取所有的新闻详情页的链接



- 分布式爬虫
    - 概念：我们需要搭建一个分布式的机群，让其对一组资源进行分布联合爬取。
    - 作用：提升爬取数据的效率

    - 如何实现分布式？
        - 安装一个scrapy-redis的组件
        - 原生的scarapy是不可以实现分布式爬虫，必须要让scrapy结合着scrapy-redis组件一起实现分布式爬虫。
        - 为什么原生的scrapy不可以实现分布式？
            - 调度器不可以被分布式机群共享
            - 管道不可以被分布式机群共享
        - scrapy-redis组件作用：
            - 可以给原生的scrapy框架提供可以被共享的管道和调度器
        - 实现流程
            - 创建一个工程
            - 创建一个基于CrawlSpider的爬虫文件
            - 修改当前的爬虫文件：
                - 导包：from scrapy_redis.spiders import RedisCrawlSpider
                - 将start_urls和allowed_domains进行注释
                - 添加一个新属性：redis_key = 'sun' 可以被共享的调度器队列的名称
                - 编写数据解析相关的操作
                - 将当前爬虫类的父类修改成RedisCrawlSpider
            - 修改配置文件settings
                - 指定使用可以被共享的管道：
                    ITEM_PIPELINES = {
                        'scrapy_redis.pipelines.RedisPipeline': 400
                    }
                - 指定调度器：
                    # 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化
                    DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
                    # 使用scrapy-redis组件自己的调度器
                    SCHEDULER = "scrapy_redis.scheduler.Scheduler"
                    # 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据
                    SCHEDULER_PERSIST = True
                - 指定redis服务器：

            - redis相关操作配置：
                - 配置redis的配置文件：
                    - linux或者mac：redis.conf
                    - windows:redis.windows.conf
                    - 代开配置文件修改：
                        - 将bind 127.0.0.1进行删除
                        - 关闭保护模式：protected-mode yes改为no
                - 结合着配置文件开启redis服务
                    - redis-server 配置文件
                - 启动客户端：
                    - redis-cli
            - 执行工程：
                - scrapy runspider xxx.py
            - 向调度器的队列中放入一个起始的url：
                - 调度器的队列在redis的客户端中
                    - lpush xxx www.xxx.com
            - 爬取到的数据存储在了redis的proName:items这个数据结构中


增量式爬虫

    - 概念：监测网站数据更新的情况，只会爬取网站最新更新出来的数据。
    - 分析：
        - 指定一个起始url
        - 基于CrawlSpider获取其他页码链接
        - 基于Rule将其他页码链接进行请求
        - 从每一个页码对应的页面源码中解析出每一个电影详情页的URL

        - 核心：检测电影详情页的url之前有没有请求过
            - 将爬取过的电影详情页的url存储
                - 存储到redis的set数据结构

        - 对详情页的url发起请求，然后解析出电影的名称和简介
        - 进行持久化存储


# requests库

requests提供的请求各个请求方式：

    import requests
    requests.get(url)
    requests.post(url)
    requests.put(url)
    requests.delete(url)
    requests.head(url)
    requests.options(url)

get请求
get请求核心代码是requests.get(url)，具体例子如下：

    import requests
    url = 'http://baidu.com'
    response = requests.get(url)
    print(response)
    
打印出来的结果是：<Response [200]>。<>表示这是一个对象，也就是我们这里获取的是一个response的对象，200表示状态码。

post请求
post请求核心代码是requests.post(url,data={请求体的字典})，具体例子（百度翻译）如下：

    import requests
    url = 'https://fanyi.baidu.com'
    data = {'from': 'zh',
        'to': 'en',
        'query': '人生苦短，我用python'
    }
    response = requests.post(url, data=data)
    print(response)

data部分的参数，取自于页面NetWork→Headers→Form Data。打印出来的结果是：<Response [200]>。


response方法

获取网页的解码字符串
通过上述例子我们可以看到，不管是get请求还是post请求，我们得到的返回都是一个Response[200]的对象，但是我们想要得到的，应该是与网页response下一样的字符串对象，这时就需要用到response的方法了。

- response.text。获取网页的HTML字符串，该方式往往会出现乱码，出现乱码使用response.encoding='utf-8'
    
        import requests
        url = 'http://baidu.com'
        response = requests.get(url)
        response.encoding = 'utf-8'
        print(response.text)
        
- response.content.decode()。把相应的二进制字节流转化为str类型。

        import requests
        url = 'http://baidu.com'
        response = requests.get(url)
        print(response.content.decode('gbk'))
        
在这里总结了一下三种获取网页源码的三种方式，通过这三种方式，一定可以获取到网页正确解码之后的字符串：

    response.content.decode()
    response.content.decode('gbk')
    response.text

获取其他属性

    import requests
    response = requests.get("http://www.baidu.com") 
    print(type(response.requests.headers),response.requests.headers) #获取请求头
    print(type(response.headers),response.headers) #获取响应头
    print(type(response.cookies),response.cookies)#获取响应cookie
    print(type(response.url),response.url) #获取响应url
    print(type(response.requests.url),response.requests.url) #获取请求url

# 发送带Headers的请求

有很多网站为了防止爬虫程序爬网站造成网站瘫痪，所以我们的程序在模拟浏览器访问这些网站时，需要携带一些headers头部信息才能访问，最常见的有User-Agent、referer、cookie参数。
写一个简单的例子：

        import requests
        url = 'https://www.zhihu.com.'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36'}
        response = requests.get(url, headers=headers)
        # response.encoding = 'utf-8'
        print(response.text)

如果User-Agent还是不能够获取正确解码后的字符串，我们还可以再headers字典中，加入referer、cookie参数。

# timeout以及retrying的使用

timeout参数的使用
在某些网络情况不好或者服务器端异常的情况会出现请求慢或者请求异常的情况，这个时候我们需要给
请求设置一个超时时间，而不是让程序一直在等待结果。写一个简单的例子：

import .requests
response = requests.get('http://baidu.com', timeout=3)
print(response.text)
该例子必须在3秒内返回响应，否则会报错。你可以尝试输入一个错误的url，或者将timeout时间调小，比如0.1，则会出现timeout的报错。

reteying库
在上述例子中，我们讲述了超时报错。在我们的程序中，如果出现了报错，我们应该是进行异常捕获。但是在实际操作中，如果你访问一个网站出现了报错，有可能是网络情况不好，这个时候我们应该是重新请求服务器，甚至是重新请求好几次。那我们在程序中可以做这个操作吗？答案是肯定的，这就是重试，也就是接下来的要讲的retrying库，在使用之前需要自己pip install retrying。写一个简单的例子：

import requests
from retrying import retry
#让被装饰的函数反复执行三次，三次全部报错才会报错，中间又一次正常都不报错
@retry(stop_max_attempt_number=3)
def parse_url(url):
    response = requests.get(url, timeout=5)
    return response.content.decode()
既然会报错，那我们就需要去捕获异常，接下来我们捕获异常，完善上述的例子。


import requests
from retrying import retry

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36'}
#让被装饰的函数反复执行三次，三次全部报错才会报错
@retry(stop_max_attempt_number=3)
def parse_url1(url):  
    print("*"*50)
    response = requests.get(url, headers=headers, timeout=5)
    return response.content.decode()

def parse_url(url):
    try:
        html_str = parse_url1(url)
    except:
        html_str = None
    return html_str


if __name__ == '__main__':
    print(parse_url('http://www.baidu.com'))
当输入正确的url地址时，程序执行一次;当输入错误的url地址时，程序retrying了3次.

# cookie相关的请求处理

针对cookie参数的处理 ，有以下两种方式。

直接携带cookie请求url地址

- cookie放在headers中。这种方式与headers中携带User-Agent一样，只需要将cookie字符串放在headers字典中即可。

        headers = {'User-Agent': '......',
                   'Cookie':'cookie字符串'}
        requests.post(url, date, headers=headers)

- cookie字典传给cookies参数。这种方式需要新增一个cookie的字典，再将该参数传给cookies参数。写一个简单的例子：Cookie:OUTFOX_SEARCH_USER_ID=-1514949692@10.169.0.83; JSESSIONID=aaasMk1xexHQo77h5hWSw; OUTFOX_SEARCH_USER_ID_NCOO=253932778.30526197; ___rl__test__cookies=1559880583877，“=”左边为参数，“=”右边为值。
Cookie_dirt = { 'OUTFOX_SEARCH_USER_ID'= '-1514949692@10.169.0.83',
                'JSESSIONID'='aaasMk1xexHQo77h5hWSw',
                'OUTFOX_SEARCH_USER_ID_NCOO'='253932778.30526197',
                '___rl__test__cookies'='1559880583877'
                }
requests.post(url, date, headers=headers, Cookie=Cookie_dirt)
使用session
假如我们现在要登陆一个网站，需要输入用户名（username）和密码（password），那么我们可以先发送一次post请求，获取到cookie，然后再携带cookie请求登陆之后的页面。使用这种方法，我们需要用的session。使用session发送一次请求，那么服务器设置在本地的Cookie，则会直接保存在session中，此时我们再用session.get直接请求登陆后的界面。写一个简单的例子如下：


import requests


#实例化session
s = requests.session()
post_url = '登录界面的url'
headers = {}
post_data = {'username': '',
             'password': ''}
s.post(post_url, headers=headers, post_data=post_data)

#再使用session请求登陆后的页面
url = '登陆后的页面url'
response = s.get(url, headers=headers)

# 数据处理方法之JSON

什么是JSON
JSON 指的是 JavaScript 对象表示法（JavaScript Object Notation），是轻量级的文本数据交换格式，且具有自我描述性，更易理解。
JSON看起来像python类型（列表，字典）的字符串。

数据处理
在之前的文章中，我们说到了怎么用response的方法，获取到网页正确解码后的字符串。如果还有不懂的，可以先阅读Python爬虫（三）Requests库。接下来以有道翻译为例子，说说怎么通过网页解码后的字符串，提取到翻译结果。

- json.loads()
首先我们先来看下，通过response.content.decode()解码之后得到的是什么类型的数据。通过打印出来type可以看到，我们解码之后得到的是str类型的数据。

若想进一步提取数据，则需要将str转换成python中的字典，python的json库为我们提供了json.loads()用于将str类型的数据转成dict。其用法为：
import json
json.loads(json字符串)
再结合上述有道翻译的例子，得到字典类型的返回结果，并提取出来翻译结果。


import requests
import json

url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36'}
data = {'i': '人生苦短，我用python',
        'from': 'AUTO',
        'to': 'AUTO',
        'smartresult': 'dict',
        'client': 'fanyideskweb',
        'salt': '15598805838805',
        'sign': '601e9410133b355529e58d23a6c60578',
        'ts': '1559880583880',
        'bv': '565657d9b2f836d2c4c3a1fd81d7b3c3;',
        'doctype': 'json',
        'version': '2.1',
        'keyfrom': 'fanyi.web',
        'action': 'FY_BY_CLICKBUTTION'
        }
response = requests.post(url, data=data, headers=headers)
html_str = response.content.decode()
#将字符串转换成字典
dict_json = json.loads(html_str)
#打印转换之后的数据以及数据类型
print(dict_json)
print(type(dict_json))
#获取翻译结果
ret = dict_json['translateResult'][0][0]['src']
print('翻译结果是：',ret)

- json.dumps()
当我们对爬虫数据进行处理时，当然不希望它们仅仅是显示出来，还希望把提取到的数据保存起来。但是当我们在把字典类型的数据写入文本时，是写入不成功的。这个时候就需要将字典类型转换成字符串，再写入到文本之中，json.dumps()的作用就是实现这一功能。
在上述例子中，我们将得到的dict_json直接写入文本，会出现如下报错：TypeError: write() argument must be str, not dict。

为了能够将字典类型的数据，转换成str字符串，则需要用json.dumps()，其用法如下：
import json
json.dumps(字典)
将上述例子的dict_json换成str字符串，再写入文本中。

import requests
import json

url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36'}
data = {'i': '人生苦短，我用python',
        'from': 'AUTO',
        'to': 'AUTO',
        'smartresult': 'dict',
        'client': 'fanyideskweb',
        'salt': '15598805838805',
        'sign': '601e9410133b355529e58d23a6c60578',
        'ts': '1559880583880',
        'bv': '565657d9b2f836d2c4c3a1fd81d7b3c3;',
        'doctype': 'json',
        'version': '2.1',
        'keyfrom': 'fanyi.web',
        'action': 'FY_BY_CLICKBUTTION'
        }
response = requests.post(url, data=data, headers=headers)
#将字符串转换成字典
dict_json = json.loads(html_str)
#将字典转换成字符串并写入fnayi.txt文件中
fanyi_str = json.dumps(dict_json)
with open('fanyi.txt', 'w') as f:
    f.write(html_str)
执行完上述的程序，会得到一个fanyi.txt的文件，其结果如下：{"type": "ZH_CN2EN", "errorCode": 0, "elapsedTime": 1, "translateResult": [[{"src": "\u4eba\u751f\u82e6\u77ed\uff0c\u6211\u7528python", "tgt": "Life is too short, I use python"}]]}。这样子的一份文档，中文部分显示的是二进制，且格式非常不利于阅读，这并不是我们想要的结果。好在json.dumps()为我们提供的两个方法，以帮助我们更好阅读文档。
1.ensure_ascii，能够让中文显示成中文；
2.indent，能够让下一行在第一行的基础上空格。
其用法如下：


fanyi_str = json.dumps(dict_json, ensure_ascii=False, indent=2)
with open('fanyi.txt', 'w', encoding='utf-8') as f:
    f.write(fanyi_str)

- json.loads()、json.load()、json.dump()以及json.dumps的区别

https://www.jianshu.com/p/28894eeb0793


爬虫数据Xpath处理步骤
lxml
在Python爬虫（七）数据处理方法之JSON中我们讲到，通过response.content.decode()解码之后得到的是str类型的数据。若想得到支持我们使用xpath进行提取数据的Element对象，则需要用到python中的另外一个库lxml。
lxml是python的一个解析库，支持HTML和XML的解析，支持XPath解析方式，而且解析效率非常高。通过lxml下的etree.HTML()对html文本进行解析，得到Element对象。其使用方法非常简单：etree.HTML(html文本)
下面通过一个简单的例子，我们来看看etree.HTML输出了什么类型数据。
from lxml import etree

url = 'http://http://fanyi.youdao.com/'
response = requests.get(url, headers=headers)
html_str = response.content.decode()
print(type(html_str))
html = etree.HTML(html_str)
print(html)
输出结果如下：<class 'str'> <Element html at 0x3290f88>，这说明了通过etree.HTML得到了html的Element对象。

Xpath提取数据
得到了Element对象之后，我们就可以利用Xpath对网页的数据进行提取。接下来我们以豆瓣书籍下的日本文学排名有例子，讲解怎么提取数据。

通过xpath helper我们自己定位到书名的xpath为//ul[@class="subject-list"]/li/div[2]/h2/a/text()。

import requests
from lxml import etree

url1 = 'https://book.douban.com/tag/%E6%97%A5%E6%9C%AC%E6%96%87%E5%AD%A6?start=0&type=T/'
headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36'}
response = requests.get(url1, headers=headers)
html_str = response.content.decode()
html = etree.HTML(html_str)
#创建xpath列表
html_name = html.xpath("//ul[@class='subject-list']/li")
for li in html_name:
    item = {}
    #获取到每一个li下的书籍名字，并去除多余的\n
    item['name'] = li.xpath('./div[2]/h2/a/text()')[0].replace("\\n", " ").strip()
    print(item)

至此，我们就爬取到了该网页下的所有书籍名字了。

- 总结
使用xpath提取爬虫数据一共就两个步骤：
1.通过etree.HTML()获取到html的Element对象。
2.通过html.xpath(xpath路径)提取到我们想要的数据。

- python爬虫常用的xpath语法

https://www.w3school.com.cn/xpath/xpath_nodes.asp

Xpath简介
XPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。
XPath基于XML的树状结构，提供在数据结构树中找寻节点的能力。它最主要的目的是为了在XML1.0或XML1.1文档节点树中定位节点所设计，它的返回值可能是节点，节点集合，原子值，以及节点和原子值的混合等。目前有XPath1.0和XPath2.0两个版本。其中Xpath1.0是1999年成为W3C标准，而XPath2.0标准的确立是在2007年。W3C关于XPath的英文详细文档请见：http://www.w3.org/TR/xpath20/ 。

chrome爬虫网页解析工具XPath helper
XPath Helper是一款Chrome浏览器的开发者插件，它支持在网页点击元素生成xpath，整个抓取使用了xpath、正则表达式、消息中间件、多线程调度框架（参考）。xpath支持列表和单节点数据获取，安装了XPath Helper后就能轻松获取HTML元素的XPath，程序员就再也不需要通过搜索html源代码，定位一些id去找到对应的位置去解析网页了。
由于现在Chrome的扩展程序已经被墙，这里给一个下载插件安装的教程链接：XPath Helper：chrome爬虫网页解析工具 Chrome插件。

python爬虫常用的xpath语法

表达式/html/body/div：表示选择html节点下的body节点下的所有子div集合
表达式./div：表示当前节点下的所有子div集合
表达式../div：表示当前节点的父节点下的所有子div集合
表达式 //：表示从任意节点下开始选择，例如：
表达式 //div：表示选择所有节点下的所有div集合
表达式/html/body//div：表示选择html节点下的body节点下的所有div集合，不仅仅包括子div集合
表达式 /html/body/div[@class= "fanyi"]：表示选择html节点下的body节点下的class= "fanyi"的div。当一个节点下有多个同名的子节点时，该方法可以精准定位到特定的节点
表达式 /html/body/div[1]：表示选择body节点下的第一个div元素
表达式 /html/body/div[last()]：表示选择body节点下的最后一个div元素
表达式 /html/body/div[last()-1]：表示选择body节点下的倒数第二个div元素
表达式 /html/body/div[position()<3]：表示选择body节点下的最前面的两个div元素
表达式 //div[@class= "input__target__bar"]/a/@href：表示选择class= "fanyi"的div下的a的href值
表达式 /a/text()：表示获取a下的文本
表达式 /a//text()：表示获取a下的所有文本，适用于a存在子节点，且子节点的文本我们也需要获取
以上这些方法，是我在写爬虫程序提取数据时最常用的用法，可以毫不夸张地说，掌握了上述的xpath方法，爬虫提取数据时，可以解决80%的问题。
0人点赞
随笔


txt文件保存
该方法主要是以列表的形式将数据存储到.txt文件中，其核心代码为

with open('save.txt', 'a', encoding='utf-8') as f:
    f.write()
    f.close()
我们以上一小节Python爬虫（八）数据处理方法之Xpath提取到的书籍名字为例子，演示怎么将数据保存到txt文件中。在本例中，是通过for循环遍历字典元素实现的。将数据以列表的形式存储到txt文件中的具体代码如下：


import requests
from lxml import etree

url1 = 'https://book.douban.com/tag/%E6%97%A5%E6%9C%AC%E6%96%87%E5%AD%A6?start=0&type=T/'
headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36'}
response = requests.get(url1, headers=headers)
html_str = response.content.decode()
html = etree.HTML(html_str)
#创建xpath列表
html_name = html.xpath("//ul[@class='subject-list']/li")
for li in html_name:
    item = {}
    #获取到每一个li下的书籍名字，并去除多余的\n
    item['name'] = li.xpath('./div[2]/h2/a/text()')[0].replace("\\n", " ").strip()
    with open('save_bookname.txt', 'a', encoding='utf-8') as f:
        f.write(json.dumps(item, ensure_ascii=False))
        #每保存完一行就换行
        f.write('\n')

Excel文件保存
将爬取到的数据存储到Excel文件中的做法和将数据存储到txt文件中类似，但是需要用到python的xlwt库，其核心代码为

#1.创建book，sheet对象
book=xlwt.Workbook('encodeing=utf-8')
sheet=book.add_sheet('表格名')
#2.创建表头
sheet.write(行，列，值）
#3.写入
sheet.write(行，列， 值）
#4.保存
book.save('文件名.xls')
我们仍然以上一小节Python爬虫（八）数据处理方法之Xpath提取到的书籍名字为例子，演示怎么将数据保存到Excel文件中。在本例中，先通过for循环将所有的数据存取到列表中，再通过for循环遍历列表元素保存到Excel文件中。具体代码如下：


import requests
from lxml import etree

url1 = 'https://book.douban.com/tag/%E6%97%A5%E6%9C%AC%E6%96%87%E5%AD%A6?start=0&type=T/'
headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36'}
response = requests.get(url1, headers=headers)
html_str = response.content.decode()
html = etree.HTML(html_str)
#创建xpath列表
html_name = html.xpath("//ul[@class='subject-list']/li")
for li in html_name:
    item = {}
    #获取到每一个li下的数据名字，并去除多余的\n
    item['name{}'] = li.xpath('./div[2]/h2/a/text()')[0].replace("\\n", " ").strip()
    namelist.append(item)
book = xlwt.Workbook(encoding='utf-8')
sheet = book.add_sheet('bookname')
sheet.write(0, 0, '书名')
i = 1
for name_dict in namelist:
    #写入书名
    sheet.write(i, 0, name_dict['name'])
    #每写完一行数据，下一次换到下一行
    i += 1
book.save('E:\ClubKingdee\save_bookname.xls')


